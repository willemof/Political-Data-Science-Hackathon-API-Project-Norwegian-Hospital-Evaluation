---
title: "Group 4 Rmarkdown report"
author: "Izolda, Christine, Hedvig, Willem, Xhensila"
date: "2022-08-01"
output:
  html_document:
    df_print: paged
editor_options:
  markdown:
    wrap: sentence
---

Text in () means that it is not supposed to end up in the report, it is the description of what we need to write about.

\section{Introduction}

(Repeat the problem statement and why it's important)

The Office of the Auditor General has assigned our group to assess the efficiency of hospitals in Norway. As the main auditing institution for the Norwegian government, it is important to see that the resources (taxes) collected is spent well. Previous investigations have indicated that there are large differences in efficiency and access to care between regions and hospitals.
Using quality indicators from the Norwegian Health Directorate we can assess quality improvements from year to year. Statistics Norway provides data on various accounts which are used as input factors for hospitals. We derive measurements of productivity from these variables.



\section{Background}

The Office of the Auditor General is interested in keeping efficiency, productivity and quality as high as possible. This is in the interest of optimizing the distribution of the state-budget. 

The data collection is done through web-APIs, API stands for Application Programming Interface which is a system, or interface, by which we can communicate with a server and ask to pull data from a hosting server. 



\section{Brainstorming}

(Describe different ideas to solve the problem statement. List pros and cons with different ideas. End with the idea you landed on and describe what it is and why you landed on this idea)

With limited time it was necessary to narrow the scope of the assignment. The Office of the Auditor General preferred to see an automated data collection system using APIs. A lot of time was spent on automating the process of obtaining data from the APIs of Statistics Norway and the Health Directorate. At some point though the we had become too dependent on the initial queries that had retrieved data.

In the Data collection section we will expand on some of the ideas behind how automated data retrieval could function.


\section{Data collection}

(Describe the data collection and data cleaning process. Where did you get the data, how did you get it and what did you have to do to make the data ready?)

#Obtaining data from Statistics Norway

Statistics Norway really tries to make obtaining data as simply as possible, there is a API console that helps find the right dataset and helps generate the correct instructions (queries) that will pull exactly the data that one wants. 

After repeatedly running this code to import the data, we started thinking that maybe it would be nice to only need to pull data once and then save the data on the computer, so as to not unnecessarily bother the servers.

There was at one time a rather grandiose plan of only needing the links to the tables which would be enough to retrieve all possible meta-data for the data set, and setting it so that there was little filtering. This would make it so that we would get a big data set saved to our computer that we could filter from within our R-script. Below is how the code looked like at one point. For simplicity sake I made each query into seperate objects, but one of the ideas was that these queries could have been values within a list that with enough if statements within a for-loop would generate a very large data set. One of the issues, which could have been solved given enough time, is that the servers won't answer queries that contain more than 800 000 cells/observations. This could have been side-stepped by simply pulling each year individually and rebuilding the data-set. 

```{r }
library(httr)
library(tidyverse)
library(httr)
library(tinytex)
library(tidyverse)
library(janitor)
library(kableExtra)
library(rjstat)
library(dplyr)
library(stringr)
library(PxWebApiData)
url_list <- c("https://data.ssb.no/api/v0/en/table/06464/",
              "https://data.ssb.no/api/v0/en/table/06922/",
              "https://data.ssb.no/api/v0/en/table/09548/")
ds_tables <- c()
for(i in 1:NROW(url_list)){
  ds_tables[i] <-  str_extract(url_list[i], "/+[0-9]+/") %>%
    str_remove_all("/")
}

  query1<-  ApiData(url_list[1], HelseReg = TRUE, 
                    HelseRegnKost = TRUE,
                    HelseRegnFunk = TRUE,
                    Tid= c(-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12),
                    ContentsCode = TRUE,
                    returnApiQuery = TRUE )
  
  query2<-  ApiData(url_list[2], HelseReg = TRUE, 
                    HelseTjenomr = TRUE,
                    Tid= c(-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12),
                    ContentsCode = TRUE,
                    returnApiQuery = TRUE )
  
  query3<-  ApiData(url_list[3], HelseReg = TRUE, 
                    HelseTjenomr = TRUE,
                    Yrke = TRUE,
                    Tid= c(-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12),
                    ContentsCode = TRUE,
                    returnApiQuery = TRUE )
  

  
  query_list <- c(query1, query2, query3)

file.list <- paste0("./data/individual_datasets/",
                    str_extract(ds_tables, "^[0-9]+"),".csv")
#for-loop generating merged ssb dataset
ssb_ds<- tibble()

{  
  if(file.exists("./data/merged_datasets/ssb_ds.csv")){
  ssb_ds<- read_csv("./data/merged_datasets/ssb_ds.csv")
} 
  if(file.exists("./data/merged_datasets/ssb_ds.csv") == FALSE){ 
  for(i in 1:NROW(url_list)){
  if(file.exists(file.list[i])){
    print(noquote(c(file.list[i],noquote("was retrieved from the project folder. No download has been done,"),  
                    noquote("because the file already exists."))))
    d.tmp.list<-read_csv(file.list[i], col_names = TRUE)
    if(i==1) {
      ssb_ds <- d.tmp.list
    }
    if(i>1) {ssb_ds<-full_join(ssb_ds, d.tmp.list)
    }
    if(i==NROW(url_list)){ssb_ds<-clean_names(ssb_ds)
    write_csv(ssb_ds, file = "./data/merged_datasets/ssb_ds.csv")
    }
    next
  }
  d.tmp.list <- c(c())
  d.tmp.list[[i]] <- POST(url_list[i],body=query_list[i],encode="json",verbose())
  d.tmp.list <- do.call(rbind, d.tmp.list)
  d.tmp.list <- POST(url_list[i], body= query_list[i],encode="json",verbose())
  d.tmp.list <- fromJSONstat(content(d.tmp.list, "text"))
  d.tmp.list <- do.call(rbind, d.tmp.list)
  write_csv(d.tmp.list, file.list[i])
  print(noquote(c(file.list[i], noquote("was saved."))))
  if(i==1) {
    ssb_ds <- d.tmp.list
  }
  if(i>1) {
    ssb_ds<-full_join(ssb_ds, d.tmp.list)
  }
  if(i==NROW(url_list)){
    
    ssb_ds<-clean_names(ssb_ds)
    
    write_csv(ssb_ds, file = "./data/merged_datasets/ssb_ds.csv")
  }
  Sys.sleep(0.1+abs(rnorm(1)))
}
}
}
 #end of if statement that generates ssb_ds

```

Fairly late into the project it became clear that though the code could be valuable in the future, it would make us have to re-filter everything anew. So other than making the queries look a little nicer, as well as saving some lines of code, the large API code was scrapped. 

``` 
# How it is now:        
url <- "https://data.ssb.no/api/v0/no/table/06464/"
data <-        ApiData(url, HelseReg = list("vs:HelseForRegn2", c("H12_R","H01_R","883971752","983971652","983971636","983971680","983971700","983971768","983971784","894166762","993467049","H02_R","883975162","883975332","987399708","983975200","983975259","983975267","983975305","983975348","983971687","983971695","H03_R","983974678","983974694","983974724","983974732","H04_R","883974832","983974759","983974767","983974791","998308615","986523065","997005562","H05_R","983974880","983974899","983974902","983974910","983974929")), 
                       HelseRegnKost = list("item", c("000")),
                       Tid= list("item", c("2010","2011","2012","2013","2014","2015","2016","2017","2018","2019","2020","2021")),
                       ContentsCode = list("item", c("LopendeKr")),
                       HelseRegnFunk = FALSE,
                       returnApiQuery = TRUE )
d.tmp <- POST(url , body = data, encode = "json", verbose())
d.tmp <- fromJSONstat(content(d.tmp, "text"))
driftskostnader_hospitals <- do.call(rbind, d.tmp)

# How it used to be:
url <- "https://data.ssb.no/api/v0/no/table/06464/"
data <- '
{
  "query": [
    {
      "code": "HelseReg",
      "selection": {
        "filter": "vs:HelseForRegn2",
        "values": [
          "H12_R",
          "H01_R",
          "883971752",
          "983971652",
          "983971636",
          "983971680",
          "983971700",
          "983971768",
          "983971784",
          "894166762",
          "993467049",
          "H02_R",
          "883975162",
          "883975332",
          "987399708",
          "983975200",
          "983975259",
          "983975267",
          "983975305",
          "983975348",
          "983971687",
          "983971695",
          "H03_R",
          "983974678",
          "983974694",
          "983974724",
          "983974732",
          "H04_R",
          "883974832",
          "983974759",
          "983974767",
          "983974791",
          "998308615",
          "986523065",
          "997005562",
          "H05_R",
          "983974880",
          "983974899",
          "983974902",
          "983974910",
          "983974929"
        ]
      }
    },
    {
      "code": "HelseRegnKost",
      "selection": {
        "filter": "item",
        "values": [
          "000"
        ]
      }
    },
    {
      "code": "ContentsCode",
      "selection": {
        "filter": "item",
        "values": [
          "LopendeKr"
        ]
      }
    },
    {
      "code": "Tid",
      "selection": {
        "filter": "item",
        "values": [
          "2010",
          "2011",
          "2012",
          "2013",
          "2014",
          "2015",
          "2016",
          "2017",
          "2018",
          "2019",
          "2020",
          "2021"
        ]
      }
    }
  ],
  "response": {
    "format": "json-stat2"
  }
}
'
d.tmp <- POST(url , body = data, encode = "json", verbose())
driftskostnader_hospitals <- fromJSONstat(content(d.tmp, "text"))
```



From Statistics Norway we retrieve data on: 
Driftskostnader - Operating Costs
fristbrudd - exceeded waste time
Avtalte årsverk - Contracted man-years

On the health region level, as well as hospital level. 

#Obtaining data from the Norwegian Health Directorate
Obtaining data from the Norwegian Health Directorate was not an equally as friendly of a user-experience. To use the Health Directorate API one needed to register an account, and generate a key to be allowed to communicate with the database. We were first and foremost interested in the health quality indicators that were available, however there were 186 quality indicators that was tied to quite specific health-care experiences. In an ideal world, we could have done our analysis on all the quality indicators if we put our analysis in an algorithm, but this would have taken more time than what we have. 

```
url <- "https://api.helsedirektoratet.no/innhold/nki/kvalitetsindikatorer/"
key <- "1c50d76931ba48f69d177c18eaf3c6a8"

#Meta-dataset 
ds<-GET(url, 
        add_headers("Ocp-Apim-Subscription-Key" = key),
        add_headers("Cache-Control"= "no-cache")) %>% 
  content(as = "text") %>% # extracting the data
  jsonlite::fromJSON(flatten = TRUE) # parsing to dataframe

#Retrieving attachments connected to ds
api.call <- do.call(rbind, ds$attachments)
api.call <- api.call %>% 
  filter(fileType == "application/json")

#for-Loop that retrieves all quality indicators and saves them to data folder
#It's a 10 minute download.
for(i in 1:nrow(api.call)){
  
  file.tmp <- paste0("./data/quality_indicators/",
                     str_extract(api.call$fileName[i], "^[0-9]+"),
                     ".csv")
  
  if(file.exists(file.tmp)){
    print(noquote(c(file.tmp, noquote("was skipped."))))
    next
  }
  
  tmp <- GET(api.call$fileUri[i],
             add_headers("Ocp-Apim-Subscription-Key" = key),
             add_headers("Cache-Control"= "no-cache")) %>% 
    content(as = "text") %>% 
    jsonlite::fromJSON(flatten = TRUE) # parsing to dataframe
  
  write_csv(tmp$AttachmentDataRows, file.tmp)
  print(noquote(c(file.tmp, noquote("was saved."))))
  Sys.sleep(2+abs(rnorm(1)))
  
}
#End of for-loop

#makes qi list out of csv files
qi_files <- list.files("./data/quality_indicators/",
                       full.names = TRUE)
#combines qis into a single dataset
qi <- lapply(qi_files, read_csv, show_col_types = FALSE)
qi <- bind_rows(qi)

```
The great thing about this code-block is that it saves the 186 quality indicators on the computer, so that if we rerun the code, it instead skip re-downloading, and thus avoids unnecessarily overloading the server. In the end we chose to look at the following quality indicators:

fristbrudd - exceeded wait-time
korridor - amount of patients who need to be treated in the corridor. measures exceeded capacity.
medvirkning - participation rate of plan process for treating psychiatric illness and drug addiction.
overlevelse - post-hospitalization survival rate
pasient erfaringer - patient experience
utsettelse - postponement/delay
reinleggelse - post-treatment hospitalization


Now that we had the data, we could then move over to wrangling the data with the goal of visualizing the development over time, as well as running a regressional analysis.



(Code from cleaning the data) Izolda

Data from SSB looked tidy and there were no missing values.
Data from Helse Direktoratet was not as tidy and needed some transforming and deleting rows with a lot of missing values.

We used the function clean_names() to make the column names' syntax consistent.
Dates from Helse Direktoratet were in a wrong format, so we had to use the format() function to change it to only year.

We filtered the data using the filter() function to get the columns with the right variables and rows with the right hospitals.
For some columns there were too few yearly observations so we removed the yearly filter and use the "tertialvis" period type and only chose the observations that are first in the year (another solution could be to use the mean() function on all the observations in the year).

Both datasets had columns that were not useful for our analysis, so we used the select() function to choose the columns with regions/hospitals names, year and value.

Then, we merged all the datasets from Helse Direktoratet and renamed the time column from "time_from" to "ar" for consistency with SSB dataset and so that it would be easier to merge the two big datasets.
For SSB dataset, we renamed the "region" column into "location_name" for the same purpose.
Some hospitals in Helse Direktoratet had too few observations so we deleted them with %notin% function.

For the health region data we devided the number of man-years by population of the health region using the mutate() function.
The number of consultations, man-years, 24-hour stays and day treatments were changed so that they would be representing the number of services per citizen of the health region.
The names of health regions were not consistent from SSB and Helse Direktoratet, so we had to mutate them so that they would be the same in both datasets.

```{r}

```

\section{Data description}

Izolda for tables, Chrisitne for plots

(Offer some tables and visualizations, using this data to illuminate the problem statement) (Code from merging data and visualisation in here)

```{r}

```

\section{Empirical Strategy} Hedvig, Christine

\section{Findings and Main Results} Hedvig, Cristine

(Present your final solution -- whether it is an indicator, a dashboard, a database, etc. Give some examples on how the solution could be used to solve the problem statement.) (Code from AIC analysis and linear regression in here)

```{r}

```

\section{Limitations and steps forward} Hedvig, Christine

Write something about what might be the limitations of the solution, and how one could go about to advance the project.

(- hard to find the population per hospital)

(-not able to make a for loop over the list of data frames)

\section{Discussion/ Conclusion}
